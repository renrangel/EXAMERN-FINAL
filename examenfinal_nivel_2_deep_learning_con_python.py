# -*- coding: utf-8 -*-
"""ExamenFinal-Nivel 2- Deep learning con Python.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17JA_uGolWfKL3T6D-L9Xirr_MftqX_N1
"""

# Commented out IPython magic to ensure Python compatibility.
#Importar librerías
import numpy as np
import pandas as pd
import seaborn as sns
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
from sklearn import metrics
from sklearn.metrics import confusion_matrix
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression 
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import MinMaxScaler 
from sklearn.cluster import KMeans
import matplotlib.gridspec as gridspec
from sklearn.ensemble import RandomForestClassifier 
# %matplotlib inline

"""
Establezca 2 modelos de clasificación para el data irs
"""

sns.set(style="white", color_codes=True)
Dataframe1=pd.read_csv('https://raw.githubusercontent.com/renrangel/EXAMERN-FINAL/main/Iris.csv')
Dataframe1
#diagrama de dispersion con matplotlib
Dataframe1.plot(kind="scatter", x="SepalLengthCm", y="SepalWidthCm")

sns.pairplot(Dataframe1.drop("Id", axis=1), hue="Species", size=2)

#diagrama de dispersion con seaborn
sns.jointplot(x="SepalLengthCm", y="SepalWidthCm", data=Dataframe1, size=5)

sns.pairplot(Dataframe1.drop("Id", axis=1), hue="Species", size=2)

#separacion de la data
X = Dataframe1.iloc[:, :-1].values
y = Dataframe1.iloc[:, -1].values

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 32)


classifier = LogisticRegression()
classifier.fit(X_train, y_train)

y_pred = classifier.predict(X_test) 

print(classification_report(y_test, y_pred))
print(confusion_matrix(y_test, y_pred)) 
print('Nivel de precisión: ',accuracy_score(y_pred,y_test))

#Modelo KNN

classifier = KNeighborsClassifier(n_neighbors=10)
classifier.fit(X_train, y_train) 

y_pred = classifier.predict(X_test)  

print(classification_report(y_test, y_pred)) 
print(confusion_matrix(y_test, y_pred))


print('Nivel de precisión: ',accuracy_score(y_pred,y_test))

"""
Evalúa 2 modelos: Uno con PCA y otro sin PCA para el modelo de clasificación del dataset melbournhouses
"""
Dataframe2 =pd.read_csv('https://raw.githubusercontent.com/pedrorotta/PythonIntermedio2022/main/Examen/melbournehouses.csv')
Dataframe2

Dataframe2.describe()

Dataframe2.plot(kind="scatter", x="Landsize", y="Price")

fig, ax = plt.subplots(figsize = (15,8))
ax.scatter(Dataframe2['Landsize'], Dataframe2['Price'])
ax.set_xlabel('LandSize')
ax.set_ylabel('Price')
plt.xlim([0,1250])
plt.ylim([0,6e6])
plt.show()

#análisis PCA
X = np.array(Dataframe2.drop(columns = ['Suburb','Address','Type','Method','SellerG','Date','BuildingArea','YearBuilt','Car','CouncilArea','Regionname']))
X[1,:]

X_Dataframe2=Dataframe2.drop(columns = ['Suburb',	'Address',	'Type', 'Method', 'SellerG', 'Date', 'BuildingArea', 'YearBuilt', 'Car', 'CouncilArea', 'Regionname'])

pd.plotting.scatter_matrix(X_Dataframe2,figsize = (30,30), diagonal = 'kde')
plt.show()

Escalador= MinMaxScaler()
X_Escal = Escalador.fit_transform(X)
print(X_Escal[0,:])

#Análisi PCA
AnalisisPCA1 = PCA(n_components=2)
AnalisisPCA1.fit(X_Escal)
XdePCA=AnalisisPCA1.transform(X_Escal)
print(np.shape(AnalisisPCA1.components_))

print(X_Escal[0,:])
print("-"*100)
print(XdePCA[0,:])

print(np.shape(XdePCA))
y=np.array(Dataframe2['Method'])
print(np.shape(y))

plt.figure(figsize = (15,10))
plt.scatter(XdePCA[:,0],XdePCA[:,1],c= 'gray',s = 20)
plt.show()

plt.figure(figsize = (15,10))
plt.scatter(XdePCA[:,0],y,c='red')
plt.scatter(XdePCA[:,1],y,c='blue')
plt.show()

#clasificación del dataset melbournhouses
X_featuresdf = X_Dataframe2
X_featuresarray=np.array(X_featuresdf)
X_feat_method = Dataframe2['Method']
X_feat_method_array = np.array(X_feat_method)

Encoder = LabelEncoder()
Encoder.fit(X_feat_method_array)
x_method_encoder = Encoder.fit_transform(X_feat_method_array)
print(x_method_encoder)

X_SinMetodo = Dataframe2.drop(columns = ['Suburb',	'Address',	'Type', 'Method', 'SellerG', 'Date', 'BuildingArea', 'YearBuilt', 'Car', 'CouncilArea', 'Regionname'])
X_SinMetodo['Método codificado'] = x_method_encoder
X_SinMetodo

Scaler = MinMaxScaler()
X_Generado = np.array(X_SinMetodo)
X_Scalad=Scaler.fit_transform(X_Generado)
print(X_Scalad)

#Codo de Jambu
In =[]
for i in range(1,11):
  k_means = KMeans(n_clusters = i)
  k_means.fit(X_Scalad)
  In.append(k_means.inertia_)
plt.plot(range(1,11),In)

k_means2 = KMeans(n_clusters = 4) 
k_means2.fit(X_Scalad)
labels = k_means2.labels_
np.shape(labels)

PCA2 = PCA(n_components = 2)
PCA2.fit(X_Scalad)
XdePCA = PCA2.fit_transform(X_Scalad)

print(np.shape(XdePCA))
print(np.shape(labels))

plt.scatter(XdePCA[:,0],XdePCA[:,1],c = labels)
plt.show()

"""
¿Existe sobreajuste al aplicar un modelo de RF con n = 200 para el modelo de wine.csv? 
"""

"""
Puedes graficar un modelo de deep leraning para la dataset de breast-cancer
"""

Dataframe4 = pd.read_csv('https://raw.githubusercontent.com/pedrorotta/PythonIntermedio2022/main/Examen/breast-cancer.csv')
Dataframe4

Dataframe4.drop('id',axis=1,inplace=True)
len(Dataframe4)

Dataframe4.diagnosis.unique()

Dataframe4['diagnosis'] = Dataframe4['diagnosis'].map({'M':1,'B':0})
Dataframe4

Dataframe4.describe()

Dataframe4.boxplot(column=['radius_mean', 'perimeter_mean'])

Dataframe4.boxplot(column=['smoothness_mean'	,'compactness_mean'])

Dataframe4.boxplot(column=['symmetry_mean','concavity_mean'])

y_=Dataframe4['diagnosis']
plt.title('Diagnosis (M=1 , B=0)')
sns.countplot(y_)
plt.show()

features_mean=list(Dataframe4.columns[1:11])
DatFMalo=Dataframe4[Dataframe4['diagnosis'] ==1]
DatFBueno=Dataframe4[Dataframe4['diagnosis'] ==0]

features_mean

trainDF, testDF = train_test_split(Dataframe4, test_size = 0.3)

def modelo_de_clasificacion(model, data, predictors, outcome):
 
  model.fit(data[predictors],data[outcome])
  predictions = model.predict(data[predictors])
  accuracy = metrics.accuracy_score(predictions,data[outcome])
  print("Nivel de precision  : %s" % "{0:.3%}".format(accuracy))
  model.fit(data[predictors],data[outcome]) 

predictor_var = features_mean 
outcome_var='diagnosis'
model = RandomForestClassifier(n_estimators=150,min_samples_split=25, max_depth=7, max_features=2) 
modelo_de_clasificacion(model, trainDF,predictor_var,outcome_var)

"""
Crea una función que aplane la ruta de una imágen
"""
def aplanadora_de_imagen():
  print('Obtener el vector aplanado de una imagen')
  Imagen=plt.imread(input('Copie la ruta de la imagen (No http): '))
  #plt.imshow(image)
  #plt.show()
  #print(np.shape(image))
  Imagen_vector=Imagen.flatten()
  print('El vector es:')
  print(np.shape(Imagen_vector))
aplanadora_de_imagen()